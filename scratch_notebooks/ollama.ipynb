{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot, PromptRecorder\n",
    "from llamabot.prompt_library.git import write_commit_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bot = SimpleBot(\n",
    "    \"You are an expert user of Git.\",\n",
    "    model_name=\"ollama_chat/llama3\",\n",
    "    temperature=0.0,\n",
    "    api_base=os.getenv(\"OLLAMA_API_BASE\"),\n",
    "    stream=True,\n",
    ")\n",
    "diff = 'diff --git a/llamabot/bot/model_dispatcher.py b/llamabot/bot/model_dispatcher.py\\nindex eab00d5..abe4d70 100644\\n--- a/llamabot/bot/model_dispatcher.py\\n+++ b/llamabot/bot/model_dispatcher.py\\n@@ -12,6 +12,7 \\n@@ from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n from langchain.callbacks.base import BaseCallbackManager\\n from time import sleep\\n from loguru import logger\\n+from functools import partial\\n \\n # get this list from: https://ollama.ai/library\\n ollama_model_keywords = [\\n@@ -62,13 +63,14 @@ def create_model(\\n     :param verbose: (LangChain config) Whether to print debug messages.\\n     :return: The model.\\n     \"\"\"\\n-    ModelClass = ChatOpenAI\\n+    # We use a `partial` here to ensure that we have the correct way of specifying\\n+    # a model name between ChatOpenAI and ChatOllama.\\n+    ModelClass = partial(ChatOpenAI, model_name=model_name)\\n     if model_name.split(\":\")[0] in ollama_model_keywords:\\n-        ModelClass = ChatOllama\\n+        ModelClass = partial(ChatOllama, model=model_name)\\n         launch_ollama(model_name, verbose=verbose)\\n \\n     return ModelClass(\\n-        model_name=model_name,\\n         temperature=temperature,\\n         streaming=streaming,\\n         verbose=verbose,'\n",
    "\n",
    "bot(write_commit_message(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptbot = SimpleBot(\n",
    "    \"You are an expert user of Git.\",\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "with pr:\n",
    "    gptbot(write_commit_message(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.prompt_manager import prompt\n",
    "\n",
    "\n",
    "@prompt\n",
    "def write_commit_message(diff):\n",
    "    \"\"\"Please write a commit message for the following diff.\n",
    "\n",
    "    {{ diff }}\n",
    "\n",
    "    Use the Conventional Commits specification to write the diff.\n",
    "    Here is the template:\n",
    "\n",
    "        <type>[optional scope]: <description>\n",
    "\n",
    "        [body detailing the changes in bullet point]\n",
    "\n",
    "        [optional footer]\n",
    "\n",
    "    The commit contains the following structural elements,\n",
    "    to communicate intent to the consumers of your library:\n",
    "\n",
    "    fix: a commit of the type fix patches a bug in your codebase\n",
    "        (this correlates with PATCH in Semantic Versioning).\n",
    "    feat: a commit of the type feat introduces a new feature to the codebase\n",
    "        (this correlates with MINOR in Semantic Versioning).\n",
    "    BREAKING CHANGE: a commit that has a footer BREAKING CHANGE:,\n",
    "        or appends a ! after the type/scope,\n",
    "        introduces a breaking API change\n",
    "        (correlating with MAJOR in Semantic Versioning).\n",
    "        A BREAKING CHANGE can be part of commits of any type.\n",
    "\n",
    "    types other than fix: and feat: are allowed,\n",
    "    for example @commitlint/config-conventional\n",
    "    (based on the Angular convention) recommends\n",
    "    build:, chore:, ci:, docs:, style:, refactor:, perf:, test:, and others.\n",
    "\n",
    "    footers other than BREAKING CHANGE: <description> may be provided\n",
    "    and follow a convention similar to git trailer format.\n",
    "\n",
    "    Additional types are not mandated by the Conventional Commits specification,\n",
    "    and have no implicit effect in Semantic Versioning\n",
    "    (unless they include a BREAKING CHANGE).\n",
    "    A scope may be provided to a commit's type,\n",
    "    to provide additional contextual information and is contained within parenthesis,\n",
    "    e.g., feat(parser): add ability to parse arrays.\n",
    "    Within the optional body section, prefer the use of bullet points.\n",
    "\n",
    "    Write only the commit message. Do not add any more text.\n",
    "    Do not include the headers [body detailing changes...] and [short explanation why...]\n",
    "    but write the content out nonetheless.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "zephyrbot(write_commit_message(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptbot(write_commit_message(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feynman = SimpleBot(\n",
    "    \"You are Richard Feynman. You will be given a difficult concept, and your task is to explain it back.\"\n",
    ")\n",
    "feynman(\n",
    "    \"Enzyme function annotation is a fundamental challenge, and numerous computational tools have been developed. However, most of these tools cannot accurately predict functional annotations, such as enzyme commission (EC) number, for less-studied proteins or those with previously uncharacterized functions or multiple activities. We present a machine learning algorithm named CLEAN (contrastive learning–enabled enzyme annotation) to assign EC numbers to enzymes with better accuracy, reliability, and sensitivity compared with the state-of-the-art tool BLASTp. The contrastive learning framework empowers CLEAN to confidently (i) annotate understudied enzymes, (ii) correct mislabeled enzymes, and (iii) identify promiscuous enzymes with two or more EC numbers—functions that we demonstrate by systematic in silico and in vitro experiments. We anticipate that this tool will be widely used for predicting the functions of uncharacterized enzymes, thereby advancing many fields, such as genomics, synthetic biology, and biocatalysis.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot = SimpleBot(\"You are Richard Feynman\", model_name=\"orca-mini\", verbose=True)\n",
    "# bot(\"Tell me about black holes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, looks like the thing works with both verbose = True and verbose = False!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try writing commit diff\n",
    "from llamabot import SimpleBot\n",
    "from llamabot.prompt_manager import prompt\n",
    "\n",
    "bot = SimpleBot(\n",
    "    \"You are an expert user of Git.\",\n",
    "    # model_name=os.getenv(\"OPENAI_DEFAULT_MODEL\", \"gpt-3.5-turbo-16k-0613\"),\n",
    "    model_name=\"codellama:13b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 'diff --git a/llamabot/bot/model_dispatcher.py b/llamabot/bot/model_dispatcher.py\\nindex eab00d5..abe4d70 100644\\n--- a/llamabot/bot/model_dispatcher.py\\n+++ b/llamabot/bot/model_dispatcher.py\\n@@ -12,6 +12,7 \\n@@ from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n from langchain.callbacks.base import BaseCallbackManager\\n from time import sleep\\n from loguru import logger\\n+from functools import partial\\n \\n # get this list from: https://ollama.ai/library\\n ollama_model_keywords = [\\n@@ -62,13 +63,14 @@ def create_model(\\n     :param verbose: (LangChain config) Whether to print debug messages.\\n     :return: The model.\\n     \"\"\"\\n-    ModelClass = ChatOpenAI\\n+    # We use a `partial` here to ensure that we have the correct way of specifying\\n+    # a model name between ChatOpenAI and ChatOllama.\\n+    ModelClass = partial(ChatOpenAI, model_name=model_name)\\n     if model_name.split(\":\")[0] in ollama_model_keywords:\\n-        ModelClass = ChatOllama\\n+        ModelClass = partial(ChatOllama, model=model_name)\\n         launch_ollama(model_name, verbose=verbose)\\n \\n     return ModelClass(\\n-        model_name=model_name,\\n         temperature=temperature,\\n         streaming=streaming,\\n         verbose=verbose,'\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llamabot.code_manipulation import get_git_diff\n",
    "# from pyprojroot import here\n",
    "\n",
    "# diff = get_git_diff(here())\n",
    "\n",
    "# print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt\n",
    "def compose_commit_message(diff, template):\n",
    "    \"\"\"Here is a git diff:\n",
    "\n",
    "    {{ diff }}\n",
    "\n",
    "    Here is the template for writing commit messages:\n",
    "\n",
    "    {{ template }}\n",
    "\n",
    "    Please compose a commit message for the provided diff in the format of the template.\n",
    "    Return only the commit message and nothing else.\n",
    "\n",
    "    [AI]: Here is a possible commit message for the provided diff:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "<type>[optional scope]: <description>\n",
    "\n",
    "[body describing in detail the changes made to the code]\n",
    "\n",
    "[optional footer(s)]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bot(compose_commit_message(diff, template))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I have new documented behaviour.\n",
    "\n",
    "If:\n",
    "\n",
    "1. The Ollama model server is running (e.g. Ollama app on macOS),\n",
    "2. The Ollama model has been downloaded,\n",
    "\n",
    "Then:\n",
    "\n",
    "1. LangChain will autolaunch the Ollama model API server (I think?)\n",
    "2. I don't need to magically launch the process within LlamaBot.\n",
    "\n",
    "Ok, that's neat, and I think I like this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dispatch\"\"\"\n",
    "\n",
    "import contextvars\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOllama, ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from llamabot.config import default_language_model\n",
    "from llamabot.recorder import autorecord\n",
    "\n",
    "prompt_recorder_var = contextvars.ContextVar(\"prompt_recorder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama\"\n",
    "temperature = 0.0\n",
    "verbose = True\n",
    "streaming = True\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    streaming=streaming,\n",
    "    verbose=verbose,\n",
    "    callback_manager=BaseCallbackManager(\n",
    "        handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_model(\n",
    "    model_name,\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Dispatch and create the right model\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class SimpleBot:\n",
    "    \"\"\"Simple Bot that is primed with a system prompt, accepts a human message,\n",
    "    and sends back a single response.\n",
    "\n",
    "    This bot does not retain chat history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt,\n",
    "        temperature=0.0,\n",
    "        model_name=\"codellama\",\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Initialize the SimpleBot.\n",
    "\n",
    "        :param system_prompt: The system prompt to use.\n",
    "        :param temperature: The model temperature to use.\n",
    "            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n",
    "            for more information.\n",
    "        :param model_name: The name of the OpenAI model to use.\n",
    "        :param streaming: (LangChain config) Whether to stream the output to stdout.\n",
    "        :param verbose: (LangChain config) Whether to print debug messages.\n",
    "        \"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model = ChatOllama(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            verbose=verbose,\n",
    "            callback_manager=BaseCallbackManager(\n",
    "                handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "            ),\n",
    "        )\n",
    "        self.chat_history = []\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, human_message: str) -> AIMessage:\n",
    "        \"\"\"Call the SimpleBot.\n",
    "\n",
    "        :param human_message: The human message to use.\n",
    "        :return: The response to the human message, primed by the system prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=human_message),\n",
    "        ]\n",
    "        response = self.model(messages)\n",
    "        self.chat_history.append(HumanMessage(content=human_message))\n",
    "        self.chat_history.append(response)\n",
    "        autorecord(human_message, response.content)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = SimpleBot(\"You are a coding expert\", model_name=\"codellama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"how do you write a FastAPI endpoint?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
